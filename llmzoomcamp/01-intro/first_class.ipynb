{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "import instructor\n",
    "import os\n",
    "import min_search\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "with open('documents.json', 'r') as file:\n",
    "    documents = json.load(file)\n",
    "    \n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "client = instructor.patch(OpenAI(), mode=instructor.Mode.TOOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'MLOps Zoomcamp FAQ\\nThe purpose of this document is to capture frequently asked technical questions.\\nWe did this for our data engineering course, and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\\nData Engineering Zoomcamp FAQ\\n[Problem description]\\n[Solution description]\\n(optional) Added by Name',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Format for questions: [Problem title]'},\n",
       " {'text': 'Approximately 3 months. For each module, about 1 week with possible deadline extensions (in total 6~9 weeks), 2 weeks for working on the capstone project and 1 week for peer review.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'What is the expected duration of this course or that for each module?'},\n",
       " {'text': 'The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded. The rest should mostly be the same.\\nAlso all of the homeworks will be changed for the 2023 cohort.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'What’s the difference between the 2023 and 2022 course?'},\n",
       " {'text': 'Yes, it will start in May 2024',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Will there be a 2024 Cohort? When will the 2024 cohort start?'},\n",
       " {'text': 'Please choose the closest one to your answer. Also do not post your answer in the course slack channel.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'What if my answer is not exactly the same as the choices presented?'},\n",
       " {'text': 'Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Are we free to choose our own topics for the final project?'},\n",
       " {'text': 'In order to obtain the certificate, completion of the final capstone project is mandatory. The completion of weekly homework assignments is optional, but they can contribute to your overall progress and ranking on the top 100 leaderboard.',\n",
       "  'section': '+-General course questions',\n",
       "  'question': 'Can I still graduate when I didn’t complete homework for week x?'},\n",
       " {'text': 'You can get a few cloud points by using kubernetes even if you deploy it only locally. Or you can use local stack too to mimic AWS\\nAdded by Ming Jun, Asked by Ben Pacheco, Answered by Alexey Grigorev',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'For the final project, is it required to be put on the cloud?'},\n",
       " {'text': 'For those who are not using VSCode (or other similar IDE), you can automate port-forwarding for Jupyter Notebook by adding the following line of code to your\\n~/.ssh/config file (under the mlops-zoomcamp host):\\nLocalForward 127.0.0.1:8899 127.0.0.1:8899\\nThen you can launch Jupyter Notebook using the following command: jupyter notebook --port=8899 --no-browser and copy paste the notebook URL into your browser.\\nAdded by Vishal',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Port-forwarding without Visual Studio'},\n",
       " {'text': 'You can install the Jupyter extension to open notebooks in VSCode.\\nAdded by Khubaib',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Opening Jupyter in VSCode'},\n",
       " {'text': 'In case one would like to set a github repository (e.g. for Homeworks), one can follow 2 great tutorials that helped a lot\\nSetting up github on AWS instance - this\\nSetting up keys on AWS instance - this\\nThen, one should be able to push to its repo\\nAdded by Daniel Hen (daniel8hen@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Configuring Github to work from the remote VM'},\n",
       " {'text': \"Faced issue while setting up JUPYTER NOTEBOOK on AWS. I was unable to access it from my desktop. (I am not using visual studio and hence faced problem)\\nRun\\njupyter notebook --generate-config\\nEdit file /home/ubuntu/.jupyter/jupyter_notebook_config.py to add following line:\\nNotebookApp.ip = '*'\\nAdded by Atul Gupta (samatul@gmail.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Opening Jupyter in AWS'},\n",
       " {'text': 'If you wish to use WSL on your windows machine, here are the setup instructions:\\nCommand: Sudo apt install wget\\nGet Anaconda download address here. wget <download address>\\nTurn on Docker Desktop WFree Download | AnacondaSL2\\nCommand: git clone <github repository address>\\nVSCODE on WSL\\nJupyter: pip3 install jupyter\\nAdded by Gregory Morris (gwm1980@gmail.com)\\nAll in all softwares at one shop:\\nYou can use anaconda which has all built in services like pycharm, jupyter\\nAdded by Khaja Zaffer (khajazaffer@aln.iseg.ulisboa.pt)\\nFor windows “wsl --install” in Powershell\\nAdded by Vadim Surin (vdmsurin@gmai.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'WSL instructions'},\n",
       " {'text': 'If you create a folder data and download datasets or raw files in your local repository. Then to push all your code to remote repository without this files or folder please use gitignore file. The simple way to create it do the following steps\\n1. Create empty .txt file (using text editor or command line)\\n2. Safe as .gitignore (. must use the dot symbol)\\n3. Add rules\\n *.parquet - to ignore all parquet files\\ndata/ - to ignore all files in folder data\\n\\nFor more pattern read GIT documentation\\nhttps://git-scm.com/docs/gitignore\\nAdded by Olga Rudakova (olgakurgan@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': '.gitignore how-to'},\n",
       " {'text': \"Make sure when you stop an EC2 instance that it actually stops (there's a meme about it somewhere). There are green circles (running), orange (stopping), and red (stopped). Always refresh the page to make sure you see the red circle and status of stopped.\\nEven when an EC2 instance is stopped, there WILL be other charges that are incurred (e.g. if you uploaded data to the EC2 instance, this data has to be stored somewhere, usually an EBS volume and this storage incurs a cost).\\nYou can set up billing alerts. (I've never done this, so no advice on how to do this).\\n(Question by: Akshit Miglani (akshit.miglani09@gmail.com) and Answer by Anna Vasylytsya)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS suggestions'},\n",
       " {'text': 'You can get invitation code by coursera and use it in account to verify it it has different characteristics.\\nI really love it\\nhttps://www.youtube.com/watch?v=h_GdX6KtXjo',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'IBM Cloud an alternative for AWS'},\n",
       " {'text': \"I am worried about the cost of keeping an AWS instance running during the course.\\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finished your work for the day.  Using that strategy, in a day with about 5 hours of work you will pay around $0.40 USD which will account for $12 USD per month, which seems to be an affordable amount.\\nYou must remember that you would have a different IP public address every time you Restart your instance, and you would need to edit your ssh Config file.  It's worth the time though.\\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded.\\nHere is a tutorial to set this up.\\nAlso, you can estimate the cost yourself, using AWS pricing calculator (to use it you don’t even need to be logged in).\\nAt the time of writing (20.05.2023) t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week that should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD\\nHere’s a link to the estimate\\nAdded by Alex Litvinov (aaalex.lit@gmail.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS costs'},\n",
       " {'text': 'For many parts - yes. Some things like kinesis are not in AWS free tier, but you can do it locally with localstack.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Is the AWS free tier enough for doing this course?'},\n",
       " {'text': 'When I click an open IP-address in an AWS EC2 instance I get an error: “This site can’t be reached”. What should I do?\\nThis ip-address is not required to be open in a browser. It is needed to connect to the running EC2 instance via terminal from your local machine or via terminal from a remote server with such command, for example if:\\nip-address is 11.111.11.111\\ndownloaded key name is razer.pem (the key should be moved to a hidden folder .ssh)\\nyour user name is user_name\\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS EC2: this site can’t be reached'},\n",
       " {'text': 'After this command `ssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX` I got this error: \"unprotected private key file\". This page (https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/) explains how to fix this error. Basically you need to change the file permissions of the key file with this command: chmod 400 ~/.ssh/razer.pem',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Unprotected private key file!'},\n",
       " {'text': 'My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS code.\\nMy config:\\n# Copy Configuration in local nano editor, then Save it!\\nHost mlops-zoomcamp                                         # ssh connection calling name\\nUser ubuntu                                             # username AWS EC2\\nHostName <instance-public-IPv4-addr>                    # Public IP, it changes when Source EC2 is turned off.\\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem   # Private SSH key file path\\nLocalForward 8888 localhost:8888                        # Connecting to a service on an internal network from the outside, static forward or set port user forward via on vscode\\nStrictHostKeyChecking no\\nAdded by Muhammed Çelik\\nThe disconnection will occur whether I SSH via WSL2 or via VS Code, and usually occurs after I run some code, i.e. “import mlflow”, so not particularly intense computation.\\nI cannot reconnect to the instance without stopping and restarting with a new IPv4 address.\\nI’ve gone through steps listed on this page: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/\\nInbound rule should allow all incoming IPs for SSH.\\nWhat I expect to happen:\\nSSH connection should remain while I’m actively using the instance, and if it does disconnect, I should be able to reconnect back.\\nSolution: sometimes the hang ups are caused by the instance running out of memory. In one instance, using EC2 feature to view screenshot of the instance as a means to troubleshoot, it was the OS out-of-memory feature which killed off some critical processes. In this case, if we can’t use a higher compute VM with more RAM, try adding a swap file, which uses the disk as RAM substitute and prevents the OOM error. Follow Ubuntu’s documentation here: https://help.ubuntu.com/community/SwapFaq.\\nAlternatively follow AWS’s own doc, which mirrors Ubuntu’s: https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS EC2 instance constantly drops SSH connection'},\n",
       " {'text': 'Everytime I restart my EC2 instance I keep getting different IP and need to update the config file manually.\\n\\nSolution: You can create a script like this to automatically update the IP address of your EC2 instance.https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'AWS EC2 IP Update'},\n",
       " {'text': 'Make sure to use an instance with enough compute capabilities such as a t2.xlarge. You can check the monitoring tab in the EC2 dashboard to monitor your instance.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'VS Code crashes when connecting to Jupyter'},\n",
       " {'text': 'Error “ValueError: X has 526 features, but LinearRegression is expecting 525 features as input.” when running your Linear Regression Model on the validation data set:\\nSolution: The DictVectorizer creates an initial mapping for the features (columns). When calling the DictVecorizer again for the validation dataset transform should be used as it will ignore features that it did not see when fit_transform was last called. E.g.\\nX_train = dv.fit_transform(train_dict)\\nX_test = dv.transform(test_dict)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'X has 526 features, but expecting 525 features'},\n",
       " {'text': 'If some dependencies are missing\\nInstall following packages\\npandas\\nmatplotlib\\nscikit-learn\\nfastparquet\\npyarrow\\nseaborn\\npip install -r requirements.txt\\nI have seen this error when using pandas.read_parquet(), the solution is to install pyarrow or fastparquet by doing !pip install pyarrow in the notebook\\nNOTE: if you’re using Conda instead of pip, install fastparquet rather than pyarrow, as it is much easier to install and it’s functionally identical to pyarrow for our needs.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Missing dependencies'},\n",
       " {'text': 'The evaluation RMSE I get doesn’t figure within the options!\\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (0≤duration≤60) and you’ll get a RMSE which is (approximately) in the options. Also don’t forget to convert the columns data types to str before using the DictVectorizer.\\nAnother option: Along with filtering outliers, additionally filter on null values by replacing them with -1.  You will get a RMSE which is (almost same as) in the options. Use ‘.round(2)’ method to round it to 2 decimal points.\\nWarning deprecation\\nThe python interpreter warning of modules that have been deprecated  and will be removed in future releases as well as making suggestion how to go about your code.\\nFor example\\nC:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\seaborn\\\\distributions.py:2619:\\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\\nwarnings.warn(msg, FutureWarning)\\nTo suppress the warnings, you can include this code at the beginning of your notebook\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'No RMSE value in the options'},\n",
       " {'text': 'sns.distplot(df_train[\"duration\"])\\nCan be replaced with\\nsns.histplot(\\ndf_train[\"duration\"] , kde=True,\\nstat=\"density\", kde_kws=dict(cut=3), bins=50,\\nalpha=.4, edgecolor=(1, 1, 1, 0.4),\\n)\\nTo get almost identical result',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'How to replace distplot with histplot'},\n",
       " {'text': 'You need to replace the capital letter “L” with a small one “l”',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': \"KeyError: 'PULocationID'  or  'DOLocationID'\"},\n",
       " {'text': 'I have faced a problem while reading the large parquet file. I tried some workarounds but they were NOT successful with Jupyter.\\nThe error message is:\\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\\nI solved it by performing the homework directly as a python script.\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)\\nYou can try using the Pyspark library\\nAnswered by kamaldeen (kamaldeen32@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Reading large parquet files'},\n",
       " {'text': 'First remove the outliers (trips with unusual duration) before plotting\\nAdded by Ibraheem Taha (ibraheemtaha91@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Distplot takes too long'},\n",
       " {'text': 'Problem: RMSE on test set was too high when hot encoding the validation set with a previously fitted OneHotEncoder(handle_unknown=’ignore’) on the training set, while DictVectorizer would yield the correct RMSE.\\nIn principle both transformers should behave identically when treating categorical features (at least in this week’s homework where we don’t have sequences of strings in each row):\\nFeatures are put into binary columns encoding their presence (1) or absence (0)\\nUnknown categories are imputed as zeroes in the hot-encoded matrix',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'RMSE on test set too high'},\n",
       " {'text': 'A: Alexey’s answer https://www.youtube.com/watch?v=8uJ36ZZr_Is&t=13s\\nIn summary,\\npd.get_dummies or OHE can come up with result in different orders and handle missing data differently, so train and val set would have different columns during train and validation\\nDictVectorizer would ignore missing (in train) and new (in val) datasets\\nOther sources:\\nhttps://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\\nhttps://scikit-learn.org/stable/modules/feature_extraction.html\\nhttps://innovation.alteryx.com/encode-smarter/\\n~ ellacharmed',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Q: Using of OneHotEncoder instead of DictVectorizer'},\n",
       " {'text': \"Why didn't get_dummies in pandas library or OneHotEncoder in scikit-learn library be used for one-hot encoding? I know OneHotEncoder is the most common and useful. One-hot coding can also be done using the eye or identity components of the NumPy library.\\nM.Sari\\nOneHotEncoder has the option to output a row column tuple matrix. DictVectorizer is a one step method to encode and support row column tuple matrix output.\\nHarinder(sudwalh@gmail.com)\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Q: Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer ?'},\n",
       " {'text': 'How to check that we removed the outliers?\\nUse the pandas function describe() which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using boolean expression, the min and max can be verified using\\ndf[‘duration’].describe()',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Clipping outliers'},\n",
       " {'text': 'pd.get_dummies and DictVectorizer both create a one-hot encoding on string values. Therefore you need to convert the values in PUlocationID and DOlocationID to string.\\nIf you convert the values in PUlocationID and DOlocationID from numeric to string, the NaN values get converted to the string \"nan\".  With DictVectorizer the RMSE is the same whether you use \"nan\" or \"-1\" as string representation for the NaN values. Therefore the representation doesn\\'t have to be \"-1\" specifically, it could also be some other string.',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding'},\n",
       " {'text': 'Problem: My LinearRegression RSME is very close to the answer but not exactly the same. Is this normal?\\nAnswer: No, LinearRegression is an deterministic model, it should always output the same results when given the same inputs.\\nAnswer:\\nCheck if you have treated the outlier properly for both train and validation sets\\nCheck if the one hot encoding has been done properly by looking at the shape of one hot encoded feature matrix. If it shows 2 features, there is wrong with one hot encoding. Hint: the drop off and pick up codes need to be converted to proper data format and then DictVectorizer is fitted.\\nHarshit Lamba (hlamba19@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Slightly different RSME'},\n",
       " {'text': 'Problem: I’m facing an extremely low RMSE score (eg: 4.3451e-6) - what shall I do?\\nAnswer: Recheck your code to see if your model is learning the target prior to making the prediction. If the target variable is passed in as a parameter while fitting the model, chances are the model would score extremely low. However, that’s not what you would want and would much like to have your model predict that. A good way to check that is to make sure your X_train doesn’t contain any part of your y_train. The same stands for validation too.\\nSnehangsu De (desnehangsu@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Extremely low RSME'},\n",
       " {'text': 'Problem: how to enable auto completion in jupyter notebook? Tab doesn’t work for me\\nSolution: !pip install --upgrade jedi==0.17.2\\nChristopher R.J.(romanjaimesc@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Enabling Auto-completion in jupyter notebook'},\n",
       " {'text': \"Problem: While following the steps in the videos you may have problems trying to download with wget the files. Usually it is a 403 error type (Forbidden access).\\nSolution: The links point to files on cloudfront.net, something like this:\\nhttps://d37ci6vzurychx.cloudfront.net/tOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet OSError: Could not open parquet input source '<Buffer>': Invalid: Parquet rip+data/green_tripdata_2021-01.parquet\\nI’m not download the dataset directly, i use dataset URL and run this in the file.\\nUpdate(27-May-2023): Vikram\\nI am able to download the data from the below link. This is from the official  NYC trip record page (https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). Copy link from page directly as the below url might get changed if the NYC decides to move away from this. Go to the page , right click and use copy link.\\nwget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\\n(Asif)\\nCopy the link address and replace the cloudfront.net part with s3.amazonaws.com/nyc-tlc/, so it looks like this:\\nhttps://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2021-01.parquet\\nMario Tormo (mario@tormo-romero.eu)\\nOSError: Could not open parquet input source '<Buffer>': Invalid: Parquet\",\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Downloading the data from the NY Taxis datasets gives error : 403 Forbidden'},\n",
       " {'text': 'Problem: PyCharm (remote) doesn’t see conda execution path. So, I cannot use conda env (which is located on a remote server).\\nSolution: In remote server in command line write “conda activate envname”, after write “which python” - it gives you python execution path. After you can use this path when you will add new interpreter in PyCharm: add local interpreter -> system interpreter -> and put the path with python.\\nSalimov Ilnaz (salimovilnaz777@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Using PyCharm & Conda env in remote development'},\n",
       " {'text': 'Problem: The output of DictVectorizer was taking up too much memory. So much so, that I couldn’t even fit the linear regression model before running out of memory on my 16 GB machine.\\nSolution: In the example for DictVectorizer in the scikit-learn website, they set the parameter “sparse” as False. Although this helps with viewing the results, this results in a lot of memory usage. The solution is to either use “sparse=True” instead, or leave it at the default which is also True.\\nAhmed Fahim (afahim03@yahoo.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Running out of memory'},\n",
       " {'text': 'Problem: For me, Installing anaconda didn’t modify the .bashrc profile. That means Anaconda env was not activated even after exiting and relaunching the unix shell.\\nSolution:\\nFor bash : Initiate conda again, which will add entries for anaconda in .bashrc file.\\n$ cd YOUR_PATH_ANACONDA/bin $ ./conda init bash\\nThat will automatically edit your .bashrc.\\nReload:\\n$ source ~/.bashrc\\nAhamed Irshad (daisyfuentesahamed@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Activating Anaconda env in .bashrc'},\n",
       " {'text': 'While working through the HW1, you will realize that the training and the validation data set feature sizes are different. I was trying to figure out why and went down the entire rabbit hole only to see that I wasn’t doing ```transform``` on the premade dictionary vectorizer instead of ```fit_transform```. You already have the dictionary vectorizer made so no need to execute the fit pipeline on the model.\\nSam Lim(changhyeonlim@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'The feature size is different for training set and validation set'},\n",
       " {'text': 'I found a good guide how to get acces to your machine again when you removed your public key.\\nUsing the following link you can go to Session Manager and log in to your instance and create public key again. https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors\\nThe main problem for me here was to get my old public key, so for doing this you should run the following command: ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\\nFor more information: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key\\nHanna Zhukavets (a.zhukovec1901@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Permission denied (publickey) Error (when you remove your public key on the AWS machine)'},\n",
       " {'text': 'Problem: The February dataset has been used as a validation/test dataset and been stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\\nAnswer: The sparsematrix result from DictVectorizer shouldn’t be turned into an ndarray. After removing that part of the code, I ended up receiving a correct result .\\nTahina Mahatoky (tahinadanny@gmail.com)',\n",
       "  'section': 'Module 1: Introduction',\n",
       "  'question': 'Overfitting: Absurdly high RMSE on the validation dataset'},\n",
       " {'text': 'more specific error line:\\nfrom sklearn.feature_extraction import DictVectorizer\\nI had this issue and to solve it I did\\n!pip install scikit-learn\\nJoel Auccapuclla (auccapuclla 2013@gmail.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Can’t import sklearn'},\n",
       " {'text': 'Problem: Localhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\\n\\nSolution: If you are on an chrome browser you need to head to `chrome://net-internals/#sockets` and press “Flush Socket Pools”',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Access Denied at Localhost:5000 - Authorization Issue'},\n",
       " {'text': \"You have something running on the 5000 port. You need to stop it.\\nAnswer: On terminal in mac .\\nRun ps -A | grep gunicorn\\nLook for the number process id which is the 1st number after running the command\\nkill 13580\\nwhere 13580  represents the process number.\\nSource\\nwarrie.warrieus@gmail.com\\nOr by executing the following command it will kill all the processes using port 5000:\\n>> sudo fuser -k 5000/tcp\\nAnswered by Vaibhav Khandelwal\\nJust execute in the command below in he command line to kill the running port\\n->> kill -9 $(ps -A | grep python | awk '{print $1}')\\nAnswered by kamaldeen (kamaldeen32@gmail.com)\\nChange to different port (5001 in this case)\\n>> mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\\nAnswered by krishna (nellaikrishna@gmail.com)\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"Connection in use: ('127.0.0.1', 5000)\"},\n",
       " {'text': 'Running python register_model.py results in the following error:\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nFull Traceback:\\nTraceback (most recent call last):\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\\nrun(args.data_path, args.top_n)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\\ntrain_and_log_model(data_path=data_path, params=run.data.params)\\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 41, in train_and_log_model\\nparams = space_eval(SPACE, params)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\\nrval = pyll.rec_eval(space, memo=memo)\\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\\nrval = scope._impls[node.name](*args, **kwargs)\\nValueError: could not convert string to float: \\'0 int\\\\n1   float\\\\n2     hyperopt_param\\\\n3       Literal{n_estimators}\\\\n4       quniform\\\\n5         Literal{10}\\\\n6         Literal{50}\\\\n7         Literal{1}\\'\\nSolution: There are two plausible errors to this. Both are in the hpo.py file where the hyper-parameter tuning is run. The objective function should look like this.\\n\\n   def objective(params):\\n# It\\'s important to set the \"with\" statement and the \"log_params\" function here\\n# in order to properly log all the runs and parameters.\\nwith mlflow.start_run():\\n# Log the parameters\\nmlflow.log_params(params)\\nrf = RandomForestRegressor(**params)\\nrf.fit(X_train, y_train)\\ny_pred = rf.predict(X_valid)\\n# Calculate and log rmse\\nrmse = mean_squared_error(y_valid, y_pred, squared=False)\\nmlflow.log_metric(\\'rmse\\', rmse)\\nIf you add the with statement before this function, and just after the following line\\nX_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\\nand you log the parameters just after the search_space dictionary is defined, like this\\nsearch_space = {....}\\n# Log the parameters\\nmlflow.log_params(search_space)\\nThen there is a risk that the parameters will be logged in group. As a result, the\\nparams = space_eval(SPACE, params)\\nregister_model.py file will receive the parameters in group, while in fact it expects to receive them one by one. Thus, make sure that the objective function looks as above.\\nAdded by Jakob Salomonsson',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Could not convert string to float - ValueError'},\n",
       " {'text': 'Make sure you launch the mlflow UI from the same directory as thec that is running the experiments (same directory that has the mlflow directory and the database that stores the experiments).\\nOr navigate to the correct directory when specifying the tracking_uri.\\nFor example:\\nIf the mlflow.db is in a subdirectory called database, the tracking uri would be ‘sqllite:///database/mlflow.db’\\nIf the mlflow.db is a directory above your current directory: the tracking uri would be ‘sqlite:///../mlflow.db’\\nAnswered by Anna Vasylytsya\\nAnother alternative is to use an absolute path to mlflow.db rather than relative path\\nAnd yet another alternative is to launch the UI from the same notebook by executing the following code cell\\nimport subprocess\\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\\nAnd then using the same MLFLOW_TRACKING_URI when initializing mlflow or the client\\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Experiment not visible in MLflow UI'},\n",
       " {'text': \"Problem:\\nGetting\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE\\nduring MLFlow's installation process, particularly while installing the Numpy package using pip\\nWhen I installed mlflow using ‘pip install mlflow’ on 27th May 2022, I got the following error while numpy was getting installed through mlflow:\\n\\nCollecting numpy\\nDownloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\\n|██████████████              \\t| 6.3 MB 107 kB/s eta 0:01:19\\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\\nnumpy from https://files.pythonhosted.org/packages/b5/50/d7978137464251c393df28fe0592fbb968110f752d66f60c7a53f7158076/numpy-1.22.4-cp310-cp310-win_amd64.whl#sha256=3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077 (from mlflow):\\nExpected sha256 3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077\\nGot    \\t15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90\\nSolution:\\nThen when I install numpy separately (and not as part of mlflow), numpy gets installed (same version), and then when I do 'pip install mlflow', it also goes through.\\nPlease note that the above may not be consistently simulatable, but please be aware of this issue that could occur during pip install of mlflow.\\nAdded by Venkat Ramakrishnan\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Hash Mismatch Error with Package Installation'},\n",
       " {'text': 'After deleting an experiment from UI, the deleted experiment still persists in the database.\\nSolution: To delete this experiment permanently, follow these steps.\\nAssuming you are using sqlite database;\\nInstall ipython sql using the following command: pip install ipython-sql\\nIn your jupyter notebook, load the SQL magic scripts with this: %load_ext sql\\nLoad the database with this: %sql sqlite:///nameofdatabase.db\\nRun the following SQL script to delete the experiment permanently: check link',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'How to Delete an Experiment Permanently from MLFlow UI'},\n",
       " {'text': 'Problem: I cloned the public repo, made edits, committed and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\\nThis is what my config looks like (in case this might be useful):\\n[core]\\nrepositoryformatversion = 0\\nfilemode = true\\nbare = false\\nlogallrefupdates = true\\nignorecase = true\\nprecomposeunicode = true\\n[remote \"origin\"]\\nurl = git@github.com:my_username/mlops-zoomcamp.git\\nfetch = +refs/heads/*:refs/remotes/origin/*\\n[branch \"main\"]\\nremote = origin\\nmerge = refs/heads/main\\nSolution: You should fork DataClubsTak’s repo instead of cloning it. On GitHub, click “Fetch and Merge” under the menu “Fetch upstream” at the main page of your own',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'How to Update Git Public Repo Without Overwriting Changes'},\n",
       " {'text': 'This is caused by ```mlflow.xgboost.autolog()``` when version 1.6.1 of xgboost\\nDowngrade to 1.6.0\\n```pip install xgboost==1.6.0``` or update requirements file with xgboost==1.6.0 instead of xgboost\\nAdded by Nakul Bajaj',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.'},\n",
       " {'text': 'Since the version 1.29 the list_experiments method was deprecated and then removed in the later version\\nYou should use search_experiments instead\\nAdded by Alex Litvinov',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"MlflowClient object has no attribute 'list_experiments'\"},\n",
       " {'text': 'Make sure `mlflow.autolog()` ( or framework-specific autolog ) written BEFORE `with mlflow.start_run()` not after.\\nAlso make sure that all dependencies for the autologger are installed, including matplotlib. A warning about uninstalled dependencies will be raised.\\nMohammed Ayoub Chettouh',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MLflow Autolog not working'},\n",
       " {'text': 'If you’re running MLflow on a remote VM, you need to forward the port too like we did in Module 1 for Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT like in the screenshot:\\nAdded by Sharon Ibejih\\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page navigate to localhost:5000 instead.',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MLflow URL (http://127.0.0.1:5000), doesn’t open.'},\n",
       " {'text': 'Got the same warning message as Warrie Warrie when using “mlflow.xgboost.autolog()”\\nIt turned out that this was just a warning message and upon checking MLflow UI (making sure that no “tag” filters were included), the model was actually automatically tracked in the MLflow.\\nAdded by Bengsoon Chuah, Asked by Warrie Warrie, Answered by Anna Vasylytsya & Ivan Starovit',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MLflow.xgboost Autolog Model Signature Failure'},\n",
       " {'text': \"mlflow.exceptions.MlflowException: Cannot set a deleted experiment 'cross-sell' as the active experiment. You can restore the experiment, or permanently delete the  experiment to create a new one.\\nThere are many options to solve in this link: https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'MlflowException: Unable to Set a Deleted Experiment'},\n",
       " {'text': 'You do not have enough disk space to install the requirements. You can either increase the base EBS volume by following this link or add an external disk to your instance and configure conda installation to happen on the external disk.\\nAbinaya Mahendiran\\nOn GCP: I added another disk to my vm and followed this guide to mount the disk. Confirm the mount by running df -H (disk free) command in bash shell. I also deleted Anaconda and instead used miniconda. I downloaded miniconda in the additional disk that I mounted and when installing miniconda, enter the path to the extra disk instead of the default disk, this way conda is installed on the extra disk.\\nYang Cao',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'No Space Left on Device - OSError[Errno 28]'},\n",
       " {'text': 'I was using an old version of sklearn due to which I got the wrong number of parameters because in the latest version min_impurity_split for randomforrestRegressor was deprecated. Had to upgrade to the latest version to get the correct number of params.',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Parameters Mismatch in Homework Q3'},\n",
       " {'text': \"Error: I installed all the libraries from the requirements.txt document in a new environment as follows:\\npip install -r requirementes.txt\\nThen when I run mlflow from my terminal like this:\\nmlflow\\nI get this error:\\nSOLUTION: You need to downgrade the version of 'protobuf' module to 3.20.x or lower. Initially, it was version=4.21, I installed protobuf==3.20\\npip install protobuf==3.20\\nAfter which I was able to run mlflow from my terminal.\\n-Submitted by Aashnna Soni\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Protobuf error when installing MLflow'},\n",
       " {'text': 'Please check your current directory while running the mlflow ui command. You need to run mlflow ui or mlflow server command in the right directory.',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Setting up Artifacts folders'},\n",
       " {'text': 'If you have problem with setting up MLflow for experiment tracking on GCP, you can check these two links:\\nhttps://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html\\nhttps://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Setting up MLflow experiment tracker on GCP'},\n",
       " {'text': 'Solution: Downgrade setuptools (I downgraded 62.3.2 -> 49.1.0)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Setuptools Replacing Distutils - MLflow Autolog Warning'},\n",
       " {'text': 'I can’t sort runs in MLFlow\\nMake sure you are in table view (not list view) in the MLflow UI.\\nAdded and Answered by Anna Vasylytsya',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Sorting runs in MLflow UI'},\n",
       " {'text': 'Problem: When I ran `$ mlflow ui` on a remote server and try to open it in my local browser I got an exception  and the page with mlflow ui wasn’t loaded.\\nSolution: You should `pip uninstall flask` on your remote server on conda env and after it install Flask `pip install Flask`. It is because the base conda env has ~flask<1.2, and when you clone it to your new work env, you are stuck with this old version.\\nAdded by Salimov Ilnaz',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch\"},\n",
       " {'text': 'Problem: After successfully installing mlflow using pip install mlflow on my Windows system, I am trying to run the mlflow ui command but it throws the following error:\\nFileNotFoundError: [WinError 2] The system cannot find the file specified\\nSolution: Add C:\\\\Users\\\\{User_Name}\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts to the PATH\\nAdded by Alex Litvinov',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified'},\n",
       " {'text': 'Running “python hpo.py --data_path=./your-path --max_evals=50” for the homework leads to the following error: TypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nFull Traceback:\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\\nrun(args.data_path, args.max_evals)\\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\\nfmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\\nreturn trials.fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\\nreturn fmin(\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\\nrval.exhaust()\\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\\nself.run(self.max_evals - n_done, block_until_done=self.asynchronous)\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'int\\'\\nSolution:\\nThe --max_evals argument in hpo.py has no defined datatype and will therefore implicitly be treated as string. It should be an integer, so that the script can work correctly. Add type=int to the argument definition:\\nparser.add_argument(\\n\"--max_evals\",\\ntype=int,\\ndefault=50,\\nhelp=\"the number of parameter evaluations for the optimizer to explore.\"\\n)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Unsupported Operand Type Error in hpo.py'},\n",
       " {'text': 'Getting the following warning when running mlflow.sklearn:\\n\\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow. […]\\nSolution: use 0.22.1 <= scikit-learn <= 1.1.0\\nReference: https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Unsupported Scikit-Learn version'},\n",
       " {'text': 'Problem: CLI commands (mlflow experiments list) do not return experiments\\nSolution description: need to set environment variable for the Tracking URI:\\n$ export MLFLOW_TRACKING_URI=http://127.0.0.1:5000\\nAdded and Answered by Dino Vitale',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Mlflow CLI does not return experiments'},\n",
       " {'text': 'Problem: After starting the tracking server, when we try to use the mlflow cli commands as listed here, most of them can’t seem to find the experiments that have been run with the tracking server\\nSolution: We need to set the environment variable MLFLOW_TRACKING_URI to the URI of the sqlite database. This is something like “export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}” . After this, we can view the experiments from the command line using commands like “mlflow experiments search”\\nEven after this commands like “mlflow gc” doesn’t seem to get the tracking uri, and they have to be passed explicitly as an argument every time the command is run.\\nAhmed Fahim (afahim03@yahoo.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Viewing MLflow Experiments using MLflow CLI'},\n",
       " {'text': 'All the experiment and other tracking information in mlflow are stored in sqllite database provided while initiating the mlflow ui command. This database can be inspected using Pycharm’s Database tab by using the SQLLite database type. Once the connection is created as below, the tables can be queried and inspected using regular SQL. The same applies for any SQL backed database such as postgres as well.\\nThis is very useful to understand the entity structure of the data being stored within mlflow and useful for any kind of systematic archiving of model tracking for longer periods.\\nAdded by Senthilkumar Gopal',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Viewing SQLlite Data Raw & Deleting Experiments Manually'},\n",
       " {'text': 'Solution : It is another way to start it for remote hosting a mlflow server. For example, if you are multiple colleagues working together on something you most likely would not run mlflow on one laptop but rather everyone would connect to the same server running mlflow\\nAnswer by Christoffer Added by Akshit Miglani (akshit.miglani09@gmail.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'What does launching the tracking server locally mean?'},\n",
       " {'text': 'Problem: parameter was not recognized during the model registry\\nSolution: parameters should be added in previous to the model registry. The parameters can be added by mlflow.log_params(params) so that the dictionary can be directly appended to the data.run.params.\\nAdded and Answered by Sam Lim',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Parameter adding in case of max_depth not recognized'},\n",
       " {'text': 'Problem: Max_depth is not recognize even when I add the mlflow.log_params\\nSolution: the mlflow.log_params(params) should be added to the hpo.py script, but if you run it it will append the new model to the previous run that doesn’t contain the parameters, you should either remove the previous experiment or change it\\nPastor Soto',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Max_depth is not recognize even when I add the mlflow.log_params'},\n",
       " {'text': \"Problem: About week_2 homework: The register_model.py  script, when I copy it into a jupyter notebook fails and spits out the following error. AttributeError: 'tuple' object has no attribute 'tb_frame'\\nSolution: remove click decorators\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': \"AttributeError: 'tuple' object has no attribute 'tb_frame'\"},\n",
       " {'text': 'Problem: when running the preprocess_data.py file you get the following error:\\n\\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\\nSolution: Go to your WandB profile (top RHS) → user settings → scroll down to “Danger Zone” and copy your API key. \\n\\nThen before running preprocess_data.py, add and run the following cell in your notebook:\\n\\n%%bash\\n\\nWandb login <YOUR_API_KEY_HERE>.\\nAdded and Answered by James Gammerman (jgammerman@gmail.com)',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'WandB API error'},\n",
       " {'text': 'Please make sure you following the order below nd enabling the autologging before constructing the dataset. If you still have this issue check that your data is in format compatible with XGBoost.\\n# Enable MLflow autologging for XGBoost\\nmlflow.xgboost.autolog()\\n# Construct your dataset\\nX_train, y_train = ...\\n# Train your XGBoost model\\nmodel = xgb.XGBRegressor(...)\\nmodel.fit(X_train, y_train)\\nAdded by Olga Rudakova',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.'},\n",
       " {'text': 'Problem\\nUsing wget command to download either data or python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a python virtual env, it did not recognize the pip command.\\nSolution: Use python -m pip, this same for any other command. Ie. python -m wget\\nAdded by Erick Calderin',\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'wget not working'},\n",
       " {'text': \"Problem: Open/run github notebook(.ipynb) directly in Google Colab\\nSolution: Change the domain from 'github.com' to 'githubtocolab.com'. The notebook will open in Google Colab.\\nOnly works with Public repo.\\nAdded by Ming Jun\\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\\nSolution: Go to the official doc.\\nAdded by Erick Calderin\",\n",
       "  'section': 'Module 2: Experiment tracking',\n",
       "  'question': 'Open/run github notebook(.ipynb) directly in Google Colab'},\n",
       " {'text': 'Problem: Someone asked why we are using this type of split approach instead of just a random split.\\nSolution: For example, I have some models at work that train on Jan 1 2020 — Aug 1 2021 time period, and then test on Aug 1 - Dec 31 2021, and finally validate on Jan - March or something\\nWe do these “out of time”  validations to do a few things:\\nCheck for seasonality of our data\\nWe know if the RMSE for Test is 5 say, and then RMSE for validation is 20, then there’s serious seasonality to the data we are looking at, and now we might change to Time Series approaches\\nIf I’m predicting on Mar 30 2023 the outcomes for the next 3 months, the “random sample” in our train/test would have caused data leakage, overfitting, and poor model performance in production. We mustn’t take information about the future and apply it to the present when we are predicting in a model context.\\nThese are two of, I think, the biggest points for why we are doing jan/feb/march. I wouldn’t do it any other way.\\nTrain: Jan\\nTest: Feb\\nValidate: March\\nThe point of validation is to report out model metrics to leadership, regulators, auditors, and record the models performance to then later analyze target drift\\nAdded by Sam LaFell\\nProblem: If you get an error while trying to run the mlflow server on AWS CLI with S3 bucket and POSTGRES database:\\nReproducible Command:\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\\nError:\\n\"urllib3 v2.0 only supports OpenSSL 1.1.1+, currently \"\\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the \\'ssl\\' module is compiled with \\'OpenSSL 1.0.2k-fips  26 Jan 2017\\'. See: https://github.com/urllib3/urllib3/issues/2168\\nSolution: Upgrade mlflow using\\nCode: pip3 install --upgrade mlflow\\nResolution: It downgrades urllib3 2.0.3 to 1.26.16 which is compatible with mlflow and ssl 1.0.2\\nInstalling collected packages: urllib3\\nAttempting uninstall: urllib3\\nFound existing installation: urllib3 2.0.3\\nUninstalling urllib3-2.0.3:\\nSuccessfully uninstalled urllib3-2.0.3\\nSuccessfully installed urllib3-1.26.16\\nAdded by Sarvesh Thakur',\n",
       "  'section': 'Module 3: Orchestration',\n",
       "  'question': 'Why do we use Jan/Feb/March for Train/Test/Validation Purposes?'},\n",
       " {'text': 'Problem description\\nSolution description\\n(optional) Added by Name',\n",
       "  'section': 'Module 3: Orchestration',\n",
       "  'question': 'Problem title'},\n",
       " {'text': 'Here',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Where is the FAQ for Prefect questions?'},\n",
       " {'text': 'Windows with AWS CLI already installed\\nAWS CLI version:\\naws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\\nExecuting\\n$(aws ecr get-login --no-include-email)\\nshows error\\naws.exe: error: argument operation: Invalid choice, valid choices are…\\nUse this command instead. More info here:\\nhttps://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html\\naws ecr get-login-password \\\\\\n--region <region> \\\\\\n| docker login \\\\\\n--username AWS \\\\\\n--password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.'},\n",
       " {'text': 'Use ` at the end of each line except the last. Note that multiline string does not need `.\\nEscape “ to “\\\\ .\\nUse $env: to create env vars (non-persistent). E.g.:\\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\\naws kinesis put-record --cli-binary-format raw-in-base64-out `\\n--stream-name $env:KINESIS_STREAM_INPUT `\\n--partition-key 1 `\\n--data \\'{\\n\\\\\"ride\\\\\": {\\n\\\\\"PULocationID\\\\\": 130,\\n\\\\\"DOLocationID\\\\\": 205,\\n\\\\\"trip_distance\\\\\": 3.66\\n},\\n\\\\\"ride_id\\\\\": 156\\n}\\'\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Multiline commands in Windows Powershell'},\n",
       " {'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\"},\n",
       " {'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"module is not available (Can't connect to HTTPS URL)\"},\n",
       " {'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"No module named 'pip._vendor.six'\"},\n",
       " {'text': 'Problem description. How can we use Jupyter notebooks with the Pipenv environment?\\nSolution: Refer to this stackoverflow question. Basically install jupyter and ipykernel using pipenv. And then register the kernel with `python -m ipykernel install --user --name=my-virtualenv-name` inside the Pipenv shell. If you are using Jupyter notebooks in VS Code, doing this will also add the virtual environment in the list of kernels.\\nAdded by Ron Medina',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Pipenv with Jupyter'},\n",
       " {'text': \"Problem: I tried to run starter notebook on pipenv environment but had issues with no output on prints. \\nI used scikit-learn==1.2.2 and python==3.10\\nTornado version was 6.3.2\\n\\nSolution: The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\\nDowngrading to tornado==6.1 fixed the issue\\nhttps://stackoverflow.com/questions/54971836/no-output-jupyter-notebook\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Pipenv with Jupyter no output'},\n",
       " {'text': 'Problem description:  You might get an error ‘Invalid base64’ after running the ‘aws kinesis put-record’ command on your local machine. This might be the case if you are using the AWS CLI version 2 (note that in the video 4.4, around 57:42, you can see a warning since the instructor is using v1 of the CLI.\\nSolution description: To get around this, pass the argument ‘--cli-binary-format raw-in-base64-out’. This will encode your data string into base64 before passing it to kinesis\\nAdded by M',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': '‘Invalid base64’ error after running `aws kinesis put-record`'},\n",
       " {'text': 'Problem description:   Running starter.ipynb in homework’s Q1 will show up this error.\\nSolution description: Update pandas (actually pandas version was the latest, but several dependencies are updated).\\nAdded by Marcos Jimenez',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.'},\n",
       " {'text': 'Use command $pipenv lock to force the creation of Pipfile.lock\\nAdded by Bijay P.',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Pipfile.lock was not created along with Pipfile'},\n",
       " {'text': 'This issue is usually due to the pythonfinder module in pipenv.\\nThe solution to this involves manually changing the scripts as describe here python_finder_fix\\nAdded by Ridwan Amure',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Permission Denied using Pipenv'},\n",
       " {'text': 'When passing arguments to a script via command line and converting it to a 4 digit number using f’{year:04d}’, this error showed up.\\nThis happens because all inputs from the command line are read as string by the script. They need to be converted to numeric/integer before transformation in fstring.\\nyear = int(sys.argv[1])\\nf’{year:04d}’\\nIf you use click library just edit a decorator\\n@click.command()\\n@click.option( \"--year\",  help=\"Year for evaluation\",   type=int)\\ndef  your_function(year):\\n<<Your code>>\\nAdded by Taras Sh',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"Error while parsing arguments via CLI  [ValueError: Unknown format code 'd' for object of type 'str']\"},\n",
       " {'text': 'Ensure the correct image is being used to derive from.\\nCopy the data from local to the docker image using the COPY command to a relative path. Using absolute paths within the image might be troublesome.\\nUse paths starting from /app and don’t forget to do WORKDIR /app before actually performing the code execution.\\nMost common commands\\nBuild container using docker build -t mlops-learn .\\nExecute the script using docker run -it --rm mlops-learn\\n<mlops-learn> is just a name used for the image and does not have any significance.',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Dockerizing tips'},\n",
       " {'text': 'If you are trying to run Flask gunicorn & MLFlow server from the same container, defining both in Dockerfile with CMD will only run MLFlow & not Flask.\\nSolution: Create separate shell script with server run commands, for eg:\\n> \\tscript1.sh\\n#!/bin/bash\\ngunicorn --bind=0.0.0.0:9696 predict:app\\nAnother script with e.g. MLFlow server:\\n>\\tscript2.sh\\n#!/bin/bash\\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\\nCreate a wrapper script to run above 2 scripts:\\n>\\twrapper_script.sh\\n#!/bin/bash\\n# Start the first process\\n./script1.sh &\\n# Start the second process\\n./script2.sh &\\n# Wait for any process to exit\\nwait -n\\n# Exit with status of process that exited first\\nexit $?\\nGive executable permissions to all scripts:\\nchmod +x *.sh\\nNow we can define last line of Dockerfile as:\\n> \\tDockerfile\\nCMD ./wrapper_script.sh\\nDont forget to expose all ports defined by services!',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Running multiple services in a Docker container'},\n",
       " {'text': 'Problem description cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1\\nSolution: you need to force and upgrade wheel and pipenv\\nJust run the command line :\\npip install --user --upgrade --upgrade-strategy eager pipenv wheel',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)'},\n",
       " {'text': \"Problem description. How can we connect s3 bucket to MLFLOW?\\nSolution: Use boto3 and AWS CLI to store access keys. The access keys are what will be used by boto3 (AWS' Python API tool) to connect with the AWS servers. If there are no Access Keys how can they make sure that they have the right to access this Bucket? Maybe you're a malicious actor (Hacker for ex). The keys must be present for boto3 to talk to the AWS servers and they will provide access to the Bucket if you possess the right permissions. You can always set the Bucket as public so anyone can access it, now you don't need access keys because AWS won't care.\\nRead more here: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\\nAdded by Akshit Miglani\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Connecting s3 bucket to MLFLOW'},\n",
       " {'text': 'Even though the upload works using aws cli and boto3 in Jupyter notebook.\\nSolution set the AWS_PROFILE environment variable (the default profile is called default)',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Uploading to s3 fails with An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"'},\n",
       " {'text': 'Problem description: lib_lightgbm.so Reason: image not found\\nSolution description: Add “RUN apt-get install libgomp1” to your docker. (change installer command based on OS)\\nAdded by Kazeem Hakeem',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Dockerizing lightgbm'},\n",
       " {'text': 'When the request is processed in lambda function, mlflow library raises:\\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module \\'dataclasses\\' has no attribute \\'__version__\\'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\\nSolution: Increase the memory of the lambda function.\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Error raised when executing mlflow’s pyfunc.load_model in lambda function.'},\n",
       " {'text': 'Just a note if you are following the video but also using the repo’s notebook The notebook is the end state of the video which eventually uses mlflow pipelines.\\nJust watch the video and be patient. Everything will work :)\\nAdded by Quinn Avila',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': '4.3 FYI Notebook is end state of Video -'},\n",
       " {'text': 'Problem description: I was having issues because my python script was not reading AWS credentials from env vars, after building the image I was running it like this:\\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\\nSolution 1:\\n\\nEnvironment Variables: \\nYou can set the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN (if you are using AWS STS) environment variables. You can set these in your shell, or you can include them in your Docker run command like this:\\nI found out by myself that those variables must be passed before specifying the name of the image, as follow:\\ndocker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\\nAdded by Erick Cal\\nSolution 2 (if AWS credentials were not found):\\nAWS Configuration Files: \\nThe AWS SDKs and CLI will check the ~/.aws/credentials and ~/.aws/config files for credentials if they exist. You can map these files into your Docker container using volumes:\\n\\ndocker run -it --rm -v ~/.aws:/root/.aws homework:v1',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'Passing envs to my docker image'},\n",
       " {'text': 'If anyone is troubleshooting or just interested in seeing the model listed on the image svizor/zoomcamp-model:mlops-3.10.0-slim.\\nCreate a dockerfile. (yep thats all) and build “docker build -t zoomcamp_test .”\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nRun “docker run -it zoomcamp_test ls /app” output -> model.bin\\nThis will list the contents of the app directory and “model.bin” should output. With this you could just copy your files, for example “copy myfile .” maybe a requirements file and this can be run for example “docker run -it myimage myscript arg1 arg2 ”. Of course keep in mind a build is needed everytime you change the Dockerfile.\\nAnother variation is to have it run when you run the docker file.\\n“””\\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\\nWORKDIR /app\\nCMD ls\\n“””\\nJust keep in mind CMD is needed because the RUN commands are used for building the image and the CMD is used at container runtime. And in your example you probably want to run a script or should we say CMD a script.\\nQuinn Avila',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'How to see the model in the docker container in app/?'},\n",
       " {'text': 'To resolve this make sure to build the docker image with the platform tag, like this:\\n“docker build -t homework:v1 --platform=linux/arm64 .”',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\"},\n",
       " {'text': \"Solution: instead of input_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'  use input_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': 'HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb'},\n",
       " {'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\"},\n",
       " {'text': 'Problem description: When running docker-compose up as shown in the video 5.2 if you go to http://localhost:3000/ you get asked for a username and a password.\\nSolution: for both of them the default is “admin”. Then you can enter your new password. \\nSee also here\\nAdded by JaimeRV',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Login window in Grafana'},\n",
       " {'text': 'Problem Description : In Linux, when starting services using docker compose up --build  as shown in video 5.2, the services won’t start and instead we get message unknown flag: --build in command prompt.\\nSolution : Since we install docker-compose separately in Linux, we have to run docker-compose up --build instead of docker compose up --build\\nAdded by Ashish Lalchandani',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Error in starting monitoring services in Linux'},\n",
       " {'text': 'Problem: When running prepare.py getting KeyError: ‘content-length’\\nSolution: From Emeli Dral:\\nIt seems to me that the link we used in prepare.py to download taxi data does not work anymore. I substituted the instruction:\\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\\nby the\\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\\nin the prepare.py and it worked for me. Hopefully, if you do the same you will be able to get those data.',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'KeyError ‘content-length’ when running prepare.py'},\n",
       " {'text': 'Problem description\\nWhen I run the command “docker-compose up –build” and send the data to the real-time prediction service. The service will return “Max retries exceeded with url: /api”.\\nIn my case it because of my evidently service exit with code 2 due to the “app.py” in evidently service cannot import “from pyarrow import parquet as pq”.\\nSolution description\\nThe first solution is just install the pyarrow module “pip install pyarrow”\\nThe second solution is restart your machine.\\nThe third solution is if the first and second one didn’t work with your machine. I found that “app.py” of evidently service didn’t use that module. So comment the pyarrow module out and the problem was solved for me.\\nAdded by Surawut Jirasaktavee',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Evidently service exit with code 2'},\n",
       " {'text': 'When using evidently if you get this error.\\nYou probably forgot to and parentheses () just and opening and closing and you are good to go.\\nQuinn Avila',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'ValueError: Incorrect item instead of a metric or metric preset was passed to Report'},\n",
       " {'text': 'You will get an error if you didn’t add a target=’duration_min’\\nIf you want to use RegressionQualityMetric() you need a target=’duration_min and you need this added to you current_data[‘duration_min’]\\nQuinn Avila',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'For the report RegressionQualityMetric()'},\n",
       " {'text': 'Problem description\\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\\nSolution description\\nThis happens because the generated data is based on an early date therefore the training dataset would be empty.\\nAdjust the following\\nbegin = datetime.datetime(202X, X, X, 0, 0)\\nAdded by Luke',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Found array with 0 sample(s)'},\n",
       " {'text': 'Problem description\\nGetting “target columns” “prediction columns” not present errors after adding a metric\\nSolution description\\nMake sure to read through the documentation on what is required or optional when adding the metric. I added DatasetCorrelationsMetric which doesn’t require any parameters because the metric evaluates for correlations among the features.\\nSam Lim',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Adding additional metric'},\n",
       " {'text': 'When you try to login in Grafana with standard requisites (admin/admin) it throw up an error.\\nAfter run grafana-cli admin reset-admin-password admin in Grafana container the problem will be fixed\\nAdded by Artem Glazkov',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Standard login in Grafana does not work'},\n",
       " {'text': 'Problem description. While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\\nSolution description. There are two things to pay attention to:\\nRefresh interval: set it to a small value: 5-10-30 seconds\\nUse your local timezone in a call to `pytz.timezone` – I couldn’t get updates before changing this from the original value “Europe/London” to my own zone',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'The chart in Grafana doesn’t get updates'},\n",
       " {'text': 'Problem description. Prefect server was not running locally, I ran `prefect server start` command but it stopped immediately..\\nSolution description. I used Prefect cloud to run the script, however I created an issue on the Prefect github.\\nBy Erick Calderin',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Prefect server was not running locally'},\n",
       " {'text': 'Solution. Using docker CLI run docker system prune to remove unused things (build cache, containers, images etc)\\nAlso, to see what’s taking space before pruning you can run docker system df\\nBy Alex Litvinov',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'no disk space left error when doing docker compose up'},\n",
       " {'text': 'Problem: when run docker-compose up –build, you may see this error. To solve, add `command: php -S 0.0.0.0:8080 -t /var/www/html` in adminer block in yml file like:\\nadminer:\\ncommand: php -S 0.0.0.0:8080 -t /var/www/html\\nimage: adminer\\n…\\nIlnaz Salimov\\nsalimovilnaz777@gmail.com',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': 'Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)'},\n",
       " {'text': 'Problem: Can we generate charts like Evidently inside Grafana?\\nSolution: In Grafana that would be a stat panel (just a number) and scatter plot panel (I believe it requires a plug-in). However, there is no native way to quickly recreate this exact Evidently dashboard. You\\'d need to make sure you have all the relevant information logged to your Grafana data source, and then design your own plots in Grafana.\\nIf you want to recreate the Evidently visualizations externally, you can export the Evidently output in JSON with include_render=True\\n(more details here https://docs.evidentlyai.com/user-guide/customization/json-dict-output) and then parse information from it for your external visualization layer. To include everything you need for non-aggregated visuals, you should also add \"raw_data\": True  option (more details here https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\\nOverall, this specific plot with under- and over-performance segments is more useful during debugging, so might be easier to access it ad hoc using Evidently.\\nAdded by Ming Jun, Asked by Luke, Answered by Elena Samuylova',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Generate Evidently Chart in Grafana'},\n",
       " {'text': \"You may get an error ‘{'errorMessage': 'Unable to locate credentials', …’ from the print statement in test_docker.py after running localstack with kinesis.\\nTo fix this, in the docker-compose.yaml file, in addition to the environment variables like AWS_DEFAULT_REGION, add two other variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Their value is not important; anything like abc will suffice\\nAdded by M\\nOther possibility is just to run\\naws --endpoint-url http://localhost:4566 configure\\nAnd providing random values for AWS Access Key ID , AWS Secret Access Key, Default region name, and Default output format.\\nAdded by M.A. Monjas\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Get an error ‘Unable to locate credentials’ after running localstack with kinesis'},\n",
       " {'text': \"You may get an error while creating a bucket with localstack and the boto3 client:\\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\\nTo fix this, instead of creating a bucket via\\ns3_client.create_bucket(Bucket='nyc-duration')\\nCreate it with\\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={\\n'LocationConstraint': AWS_DEFAULT_REGION})\\nyam\\nAdded by M\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Get an error ‘ unspecified location constraint is incompatible ’'},\n",
       " {'text': 'When executing an AWS CLI command (e.g., aws s3 ls), you can get the error <botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>.\\nTo fix it, simply set the AWS CLI environment variables:\\nexport AWS_DEFAULT_REGION=eu-west-1\\nexport AWS_ACCESS_KEY_ID=foobar\\nexport AWS_SECRET_ACCESS_KEY=foobar\\nTheir value is not important; anything would be ok.\\nAdded by Giovanni Pecoraro',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Get an error “<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>” after running an AWS CLI command'},\n",
       " {'text': 'At every commit the above error is thrown and no pre-commit hooks are ran.\\nMake sure the indentation in .pre-commit-config.yaml is correct. Especially the 4 spaces ahead of every `repo` statement\\nAdded by M. Ayoub C.',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pre-commit triggers an error at every commit: “mapping values are not allowed in this context”'},\n",
       " {'text': 'No option to remove pytest test\\nRemove .vscode folder located on the folder you previously used for testing, e.g. folder code (from week6-best-practices) was chosen to test, so you may remove .vscode inside the folder.\\nAdded by Rizdi Aprilian',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Could not reconfigure pytest from zero after getting done with previous folder'},\n",
       " {'text': 'Problem description\\nFollowing video 6.3, at minute 11:23, get records command returns empty Records.\\nSolution description\\nAdd --no-sign-request to Kinesis get records call:\\n aws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator […] --no-sign-request',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Empty Records in Kinesis Get Records with LocalStack'},\n",
       " {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\nAn error has occurred: InvalidConfigError:\\n==> File .pre-commit-config.yaml\\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\\nSolution description\\nSet uft-8 encoding when creating the pre-commit yaml file:\\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\\nAdded by MarcosMJD\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file'},\n",
       " {'text': \"Problem description\\ngit commit -m 'Updated xxxxxx'\\n[INFO] Initializing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Installing environment for https://github.com/pre-commit/pre-commit-hooks.\\n[INFO] Once installed this environment will be reused.\\nAn unexpected error has occurred: CalledProcessError: command:\\n…\\nreturn code: 1\\nexpected return code: 0\\nstdout:\\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\\nSolution description\\nClear app-data of the virtualenv\\npython -m virtualenv api -vvv --reset-app-data\\nAdded by MarcosMJD\",\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': \"Git commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'\"},\n",
       " {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\nWhen running python test_model_service.py from the sources directory, it works.\\nWhen running pytest ./test/unit_tests fails. ‘No module named ‘production’’\\nSolution description\\nUse python -m pytest ./test/unit_tests\\nExplanation: pytest does not add to the sys.path the path where pytest is run.\\nYou can run python -m pytest, or alternatively export PYTHONPATH=. Before executing pytest\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pytest error ‘module not found’ when if using custom packages in the source code'},\n",
       " {'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code'},\n",
       " {'text': 'Problem description\\nThis is the step in the ci yml file definition:\\n- name: Run Unit Tests\\nworking-directory: \"sources\"\\nrun: ./tests/unit_tests/run.sh\\nWhen executing github ci action, error raises:\\n…/tests/unit_test/run.sh Permission error\\nError: Process completed with error code 126\\nSolution description\\nAdd execution  permission to the script and commit+push:\\ngit update-index --chmod=+x .\\\\sources\\\\tests\\\\unit_tests\\\\run.sh\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Github actions: Permission denied error when executing script file'},\n",
       " {'text': 'Problem description\\nWhen a docker-compose file contains a lot of containers, running the containers may take too much resource. There is a need to easily select only a group of containers while ignoring irrelevant containers during testing.\\nSolution description\\nAdd profiles: [“profile_name”] in the service definition.\\nWhen starting up the service, add `--profile profile_name` in the command.\\nAdded by Ammar Chalifah',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Managing Multiple Docker Containers with docker-compose profile'},\n",
       " {'text': 'Problem description\\nIf you are having problems with the integration tests and kinesis double check that your aws regions match on the docker-compose and local config. Otherwise you will be creating a stream in the wrong region\\nSolution description\\nFor example set ~/.aws/config region = us-east-1 and the docker-compose.yaml - AWS_DEFAULT_REGION=us-east-1\\nAdded by Quinn Avila',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'AWS regions need to match docker-compose'},\n",
       " {'text': 'Problem description\\nPre-commit command was failing with isort repo.\\nSolution description\\nSet version to 5.12.0\\nAdded by Erick Calderin',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Isort Pre-commit'},\n",
       " {'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'How to destroy infrastructure created via GitHub Actions'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[2]['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [dict(doc_dict, course=doc_dicts['course']) for doc_dicts in documents for doc_dict in doc_dicts['documents']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = min_search.Index(\n",
    "\n",
    "    text_fields= ['question', 'text','section'],\n",
    "    keyword_fields = ['course']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<min_search.Index at 0x7fb8df4fd1e0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = { \n",
    "    'question': 3.0, \n",
    "    'section': 0.5\n",
    "    }\n",
    "\n",
    "index.search(\n",
    "    filter_dict={'course':'data-engineering-zoomcamp'},\n",
    "    query = 'Can I still join the course after the start date?',\n",
    "    boost_dict=boost,\n",
    "    num_results = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"The course has already started?\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To help you better, could you please specify which course you are referring to? If you mention the name of the course or the institution offering it, I can provide more accurate information.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseModel(BaseModel):\n",
    "    question: str = Field(..., description='The question to answer')\n",
    "    answer: str = Field(..., description='The answer to the question')\n",
    "    section: str = Field(..., description='The information information you took to answer the question')\n",
    "    confidence: str = Field(..., description='The confidence of the answer in %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"The course has already started?\"}\n",
    "    ], \n",
    "    response_model = ResponseModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Role: You are an AI course assistant programmed to answer questions based solely on provided reference material.\n",
    "\n",
    "Input:\n",
    "Question: {question}\n",
    "Reference Material: {context}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the question and reference material thoroughly.\n",
    "2. Extract only relevant facts from the reference material that directly address the question.\n",
    "3. Construct a clear, concise answer using these facts.\n",
    "4. Identify the specific section or part of the reference material used for the answer.\n",
    "5. Assess your confidence in the answer on a scale of 0 to 100.\n",
    "6. If confidence is below 60 or critical information is missing, set answer to \"Insufficient information to provide a confident response.\"\n",
    "7. Double-check that your response uses NO external knowledge or assumptions.\n",
    "\n",
    "Output your response in this exact JSON format:\n",
    "{{\n",
    "    \"question\": \"{question}\",\n",
    "    \"answer\": \"Your response based strictly on the reference material\",\n",
    "    \"section\": \"Specific section or part of the reference material used\",\n",
    "    \"confidence\": Numerical value between 0 and 100 based on the information provided on the material, if the question is not possible to be infered from the material the confidence should be 0 other wise increase given the information provided\n",
    "}}\n",
    "\n",
    "Validation:\n",
    "- Ensure all JSON fields are present and correctly formatted.\n",
    "- Verify that the confidence value is a number between 0 and 100.\n",
    "- Confirm that the answer field contains no external information.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course_response( question: str, *, index: min_search.Index = index, course: str = 'data-engineering-zoomcamp', boost: dict = boost) -> ResponseModel:\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in index.search(filter_dict={'course': course}, query=question, boost_dict=boost):\n",
    "        context += f\"Section: {doc['section']}\\nQuestion: {doc['question']}\\nAnswer: {doc['text']}\\n\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about courses\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                question=question,\n",
    "                context=context\n",
    "            )\n",
    "                }\n",
    "        ],\n",
    "        response_model=ResponseModel\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "# response = get_course_response(client, index, \"Can I still join the course after the start date?\", \"data-engineering-zoomcamp\", boost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseModel(question='Are you a pussy?', answer='Insufficient information to provide a confident response.', section='General course-related questions', confidence='0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_course_response(\"Are you a pussy?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseModel(BaseModel):\n",
    "    question: str = Field(..., description='The question to answer')\n",
    "    answer: str = Field(..., description='The answer to the question')\n",
    "    section: str = Field(..., description='The information information you took to answer the question')\n",
    "    confidence: str|int = Field(..., description='The confidence of the answer in %')\n",
    "\n",
    "\n",
    "def search( question: str, *,\n",
    "            index: min_search.Index = index, \n",
    "            course: str = 'data-engineering-zoomcamp', \n",
    "            boost: dict = { \n",
    "                'question': 3.0, \n",
    "                'section': 0.5\n",
    "    }\n",
    "    \n",
    "    ) -> ResponseModel:\n",
    "    index.search(\n",
    "    filter_dict={'course':course},\n",
    "    query = question,\n",
    "    boost_dict=boost,\n",
    "    num_results = 3\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in index.search(filter_dict={'course': course}, query=question, boost_dict=boost):\n",
    "        context += f\"Section: {doc['section']}\\nQuestion: {doc['question']}\\nAnswer: {doc['text']}\\n\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about courses\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                question=question,\n",
    "                context=context\n",
    "            )\n",
    "                }\n",
    "        ],\n",
    "        response_model=ResponseModel\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseModel(question='How do I run kafka?', answer='Insufficient information to provide a confident response.', section='None', confidence=0)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"How do I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Error while running python3 stream.py worker',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"✅SOLUTION: pip install confluent-kafka[avro].\\nFor some reason, Conda also doesn't include this when installing confluent-kafka via pip.\\nMore sources on Anaconda and confluent-kafka issues:\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/590\\nhttps://github.com/confluentinc/confluent-kafka-python/issues/1221\\nhttps://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': \"ModuleNotFoundError: No module named 'avro'\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(\"How do I run kafka?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a total modular approach\n",
    "def search_results(docs:list[dict],*,\n",
    "                   text_fields:list[str]=['question', 'text','section'],\n",
    "                   keyword_fields:list[str]=['course'],\n",
    "                   **kwargs):\n",
    "    index = min_search.Index(\n",
    "\n",
    "    text_fields= text_fields,\n",
    "    keyword_fields = keyword_fields\n",
    ")\n",
    "    index.fit(docs)\n",
    "    def search(query: str):\n",
    "        return index.search(query,**kwargs)\n",
    "    return search\n",
    "\n",
    "\n",
    "boost = { \n",
    "    'question': 3.0, \n",
    "    'section': 0.5\n",
    "    }\n",
    "\n",
    "index = min_search.Index(\n",
    "\n",
    "    text_fields= ['question', 'text','section'],\n",
    "    keyword_fields = ['course']\n",
    ")\n",
    "index.fit(docs)\n",
    "\n",
    "boost = { \n",
    "    'question': 3.0, \n",
    "    'section': 0.5\n",
    "    }\n",
    "\n",
    "index.search(\n",
    "    filter_dict={'course':'data-engineering-zoomcamp'},\n",
    "    query = 'Can I still join the course after the start date?',\n",
    "    boost_dict=boost,\n",
    "    num_results = 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = search_results(docs,boost_dict=boost,filter_dict={'course':'data-engineering-zoomcamp'},num_results=3, text_fields=['question', 'text','section'],keyword_fields=['course'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = min_search.Index(\n",
    "\n",
    "    text_fields= ['question', 'text','section'],\n",
    "    keyword_fields = ['course']\n",
    ")\n",
    "index.fit(docs)\n",
    "\n",
    "boost = { \n",
    "    'question': 3.0, \n",
    "    'section': 0.5\n",
    "    }\n",
    "\n",
    "index.search(\n",
    "    filter_dict={'course':'data-engineering-zoomcamp'},\n",
    "    query = 'Can I still join the course after the start date?',\n",
    "    boost_dict=boost,\n",
    "    num_results = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Create a new branch for development, then you can merge it to the main branch\\nCreate a new branch and switch to this branch. It allows you to make changes. Then you can commit and push the changes to the “main” branch.',\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"Dbt+git - It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. When you want to read a Gzip compressed CSV file using Pandas, you can use the read_csv() function, which is specifically designed to read CSV files. The read_csv() function accepts several parameters, including a file path or a file-like object. To read a Gzip compressed CSV file, you can pass the file path of the \".csv.gz\" file as an argument to the read_csv() function.\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\ndf = pd.read_csv(\\'file.csv.gz\\'\\n, compression=\\'gzip\\'\\n, low_memory=False\\n)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': 'Python - Pandas can read *.csv.gzip',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s('Can i kill a dog?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseModel(BaseModel):\n",
    "    question: str = Field(..., description='The question to answer')\n",
    "    answer: str = Field(..., description='The answer to the question')\n",
    "    section: str = Field(..., description='The information information you took to answer the question')\n",
    "    confidence: str|int = Field(..., description='The confidence of the answer in %')\n",
    "\n",
    "def build_prompt(question:str,context:str) -> str:\n",
    "    prompt_template = \"\"\"\n",
    "Role: You are an AI course assistant programmed to answer questions based solely on provided reference material.\n",
    "\n",
    "Input:\n",
    "Question: {question}\n",
    "Reference Material: {context}\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the question and reference material thoroughly.\n",
    "2. Extract only relevant facts from the reference material that directly address the question.\n",
    "3. Construct a clear, concise answer using these facts.\n",
    "4. Identify the specific section or part of the reference material used for the answer.\n",
    "5. Assess your confidence in the answer on a scale of 0 to 100.\n",
    "6. If confidence is below 60 or critical information is missing, set answer to \"Insufficient information to provide a confident response.\"\n",
    "7. Double-check that your response uses NO external knowledge or assumptions.\n",
    "\n",
    "Output your response in this exact JSON format:\n",
    "{{\n",
    "    \"question\": \"{question}\",\n",
    "    \"answer\": \"Your response based strictly on the reference material\",\n",
    "    \"section\": \"Specific section or part of the reference material used\",\n",
    "    \"confidence\": Numerical value between 0 and 100 based on the information provided on the material, if the question is not possible to be infered from the material the confidence should be 0 other wise increase given the information provided\n",
    "}}\n",
    "\n",
    "Validation:\n",
    "- Ensure all JSON fields are present and correctly formatted.\n",
    "- Verify that the confidence value is a number between 0 and 100.\n",
    "- Confirm that the answer field contains no external information.\n",
    "\n",
    "\"\"\"\n",
    "    return prompt_template.format(\n",
    "                question=question,\n",
    "                context=context\n",
    "            )\n",
    "\n",
    "\n",
    "def llm_answer(prompt:str) -> dict[str,str]:\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about courses\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_model=ResponseModel\n",
    "    )\n",
    "    return response.model_dump()\n",
    "\n",
    "def search( question: str, *,\n",
    "            index: min_search.Index = index, \n",
    "            course: str = 'data-engineering-zoomcamp', \n",
    "            boost: dict = { \n",
    "                'question': 3.0, \n",
    "                'section': 0.5\n",
    "    }\n",
    "    \n",
    "    ) -> ResponseModel:\n",
    "    index.search(\n",
    "    filter_dict={'course':course},\n",
    "    query = question,\n",
    "    boost_dict=boost,\n",
    "    num_results = 3\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in index.search(filter_dict={'course': course}, query=question, boost_dict=boost):\n",
    "        context += f\"Section: {doc['section']}\\nQuestion: {doc['question']}\\nAnswer: {doc['text']}\\n\"\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions about courses\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                question=question,\n",
    "                context=context\n",
    "            )\n",
    "                }\n",
    "        ],\n",
    "        response_model=ResponseModel\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = search_results(docs,boost_dict=boost,filter_dict={'course':'data-engineering-zoomcamp'},num_results=3, text_fields=['question', 'text','section'],keyword_fields=['course'])\n",
    "def rag(query:str)-> dict:\n",
    "    s = search(query)\n",
    "    prompt = build_prompt(query,s)\n",
    "    return llm_answer(prompt)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Can I still join the course after the start date?',\n",
       " 'answer': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'confidence': 100}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Can I still join the course after the start date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "BadRequestError(400, 'resource_already_exists_exception', 'index [course-questions/fhVdWuq4TGqznnoqfgjfZA] already exists')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 18\u001b[0m\n\u001b[1;32m      1\u001b[0m index_settings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_shards\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     }\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcourse-questions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mes_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llmzoomcamp-dA_2gPCh-py3.10/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py:446\u001b[0m, in \u001b[0;36m_rewrite_parameters.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llmzoomcamp-dA_2gPCh-py3.10/lib/python3.10/site-packages/elasticsearch/_sync/client/indices.py:553\u001b[0m, in \u001b[0;36mIndicesClient.create\u001b[0;34m(self, index, aliases, error_trace, filter_path, human, mappings, master_timeout, pretty, settings, timeout, wait_for_active_shards, body)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m __body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     __headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m__path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices.create\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__path_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llmzoomcamp-dA_2gPCh-py3.10/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:423\u001b[0m, in \u001b[0;36mNamespacedClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    412\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;66;03m# Use the internal clients .perform_request() implementation\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# so we take advantage of their transport options.\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_parts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llmzoomcamp-dA_2gPCh-py3.10/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:271\u001b[0m, in \u001b[0;36mBaseClient.perform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_request\u001b[39m(\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    257\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     path_parts: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    265\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ApiResponse[Any]:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_otel\u001b[38;5;241m.\u001b[39mspan(\n\u001b[1;32m    267\u001b[0m         method,\n\u001b[1;32m    268\u001b[0m         endpoint_id\u001b[38;5;241m=\u001b[39mendpoint_id,\n\u001b[1;32m    269\u001b[0m         path_parts\u001b[38;5;241m=\u001b[39mpath_parts \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    270\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m otel_span:\n\u001b[0;32m--> 271\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perform_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43motel_span\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43motel_span\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m         otel_span\u001b[38;5;241m.\u001b[39mset_elastic_cloud_metadata(response\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llmzoomcamp-dA_2gPCh-py3.10/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py:352\u001b[0m, in \u001b[0;36mBaseClient._perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    350\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTP_EXCEPTIONS\u001b[38;5;241m.\u001b[39mget(meta\u001b[38;5;241m.\u001b[39mstatus, ApiError)(\n\u001b[1;32m    353\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage, meta\u001b[38;5;241m=\u001b[39mmeta, body\u001b[38;5;241m=\u001b[39mresp_body\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# 'X-Elastic-Product: Elasticsearch' should be on every 2XX response.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# If the header is set we mark the server as verified.\u001b[39;00m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'resource_already_exists_exception', 'index [course-questions/fhVdWuq4TGqznnoqfgjfZA] already exists')"
     ]
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andresckamilo/.cache/pypoetry/virtualenvs/llmzoomcamp-dA_2gPCh-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Seed files loaded from directory with name ‘se...</td>\n",
       "      <td>Module 4: analytics engineering with dbt</td>\n",
       "      <td>When loading github repo raise exception that ...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Instead use the method “.get_feature_names_out...</td>\n",
       "      <td>3. Machine Learning for Classification</td>\n",
       "      <td>FutureWarning: Function get_feature_names is d...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>Problem description: I have one column day_of_...</td>\n",
       "      <td>Miscellaneous</td>\n",
       "      <td>Getting day of the year from day and month column</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Use both repartition and coalesce, like so:\\nd...</td>\n",
       "      <td>Module 5: pyspark</td>\n",
       "      <td>Repartition the Dataframe to 6 partitions usin...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Answer: The 2022 NYC taxi data parquet files a...</td>\n",
       "      <td>error: Error while reading table: trips_data_a...</td>\n",
       "      <td>Question: for homework 3 , we need all 12 parq...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "282  Seed files loaded from directory with name ‘se...   \n",
       "531  Instead use the method “.get_feature_names_out...   \n",
       "790  Problem description: I have one column day_of_...   \n",
       "360  Use both repartition and coalesce, like so:\\nd...   \n",
       "231  Answer: The 2022 NYC taxi data parquet files a...   \n",
       "\n",
       "                                               section  \\\n",
       "282           Module 4: analytics engineering with dbt   \n",
       "531             3. Machine Learning for Classification   \n",
       "790                                      Miscellaneous   \n",
       "360                                  Module 5: pyspark   \n",
       "231  error: Error while reading table: trips_data_a...   \n",
       "\n",
       "                                              question  \\\n",
       "282  When loading github repo raise exception that ...   \n",
       "531  FutureWarning: Function get_feature_names is d...   \n",
       "790  Getting day of the year from day and month column   \n",
       "360  Repartition the Dataframe to 6 partitions usin...   \n",
       "231  Question: for homework 3 , we need all 12 parq...   \n",
       "\n",
       "                        course  \n",
       "282  data-engineering-zoomcamp  \n",
       "531  machine-learning-zoomcamp  \n",
       "790  machine-learning-zoomcamp  \n",
       "360  data-engineering-zoomcamp  \n",
       "231  data-engineering-zoomcamp  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(documents).sample(n=10).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The purpose of this document is to capture fre...   \n",
       "1  GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2  Yes, even if you don't register, you're still ...   \n",
       "3  You don't need it. You're accepted. You can al...   \n",
       "4  You can start by installing and setting up all...   \n",
       "\n",
       "                            section  \\\n",
       "0  General course-related questions   \n",
       "1  General course-related questions   \n",
       "2  General course-related questions   \n",
       "3  General course-related questions   \n",
       "4  General course-related questions   \n",
       "\n",
       "                                            question  \\\n",
       "0               Course - When will the course start?   \n",
       "1  Course - What are the prerequisites for this c...   \n",
       "2  Course - Can I still join the course after the...   \n",
       "3  Course - I have registered for the Data Engine...   \n",
       "4   Course - What can I do before the course starts?   \n",
       "\n",
       "                      course  \n",
       "0  data-engineering-zoomcamp  \n",
       "1  data-engineering-zoomcamp  \n",
       "2  data-engineering-zoomcamp  \n",
       "3  data-engineering-zoomcamp  \n",
       "4  data-engineering-zoomcamp  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(docs).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name,document=documents )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just discovered the course. Can I still join?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query:str):\n",
    "    search_query = {\n",
    "        \"size\": 3,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^4\", \"text\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": {\n",
    "                        \"course\": \"machine-learning-zoomcamp\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = es_client.search(index=index_name,body=search_query)\n",
    "    return response['hits']['hits']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I execute a command in a running docker container?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = elastic_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'course-questions',\n",
       "  '_id': 'YgnVbpABYTodIUsMPGRb',\n",
       "  '_score': 84.67765,\n",
       "  '_source': {'text': 'Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\\ndocker run -it --entrypoint bash <image>\\nIf the container is already running, execute a command in the specific container:\\ndocker ps (find the container-id)\\ndocker exec -it <container-id> bash\\n(Marcos MJD)',\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'question': 'How do I debug a docker container?',\n",
       "   'course': 'machine-learning-zoomcamp'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'gQnVbpABYTodIUsMPWQv',\n",
       "  '_score': 51.6054,\n",
       "  '_source': {'text': \"You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\\nTo copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\\nHrithik Kumar Advani\",\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'question': 'How do I copy files from my local machine to docker container?',\n",
       "   'course': 'machine-learning-zoomcamp'}},\n",
       " {'_index': 'course-questions',\n",
       "  '_id': 'ggnVbpABYTodIUsMPWQ1',\n",
       "  '_score': 49.90638,\n",
       "  '_source': {'text': 'You can copy files from your local machine into a Docker container using the docker cp command. Here\\'s how to do it:\\nIn the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGopakumar Gopinathan',\n",
       "   'section': '5. Deploying Machine Learning Models',\n",
       "   'question': 'How do I copy files from a different folder into docker container’s working directory?',\n",
       "   'course': 'machine-learning-zoomcamp'}}]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "\n",
    "for retrieval in abc:\n",
    "    source = retrieval['_source']\n",
    "    context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n",
    "    context += context_template.format(question=source['question'], text=source['text']).strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=query, context=context).strip()\n",
    "len(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tiktoken' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tiktoken' is not defined"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    s = elastic_search(query)\n",
    "    prompt = build_prompt(query, s)\n",
    "    return llm_answer(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I just discovered the course. Can I still join?',\n",
       " 'answer': \"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'confidence': 100}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmzoomcamp-dA_2gPCh-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
